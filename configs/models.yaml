llm:
  # provider: ollama         # ollama | openai
  # model: llama3            # used if provider=ollama
  provider: openai    # openai | ollama
  model: gpt-4o-mini     # used if provider=openai
  temperature: 0.2

embeddings:
  model_name: sentence-transformers/all-MiniLM-L6-v2
  device: cpu

retrieval:
  chunk_size: 800
  chunk_overlap: 120
  top_k: 5              # chunks we feed to the LLM
  use_rerank: false
  citations_k: 3        # MAX number of URLs to show
  max_chunks_per_url: 2 # diversify context; donâ€™t take all chunks from one page
  candidate_multiplier: 3  # retrieve this many * top_k from FAISS, then filter
